[2023-01-15T03:10:42.161+0200] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: railway_v01.create_spark_engine manual__2023-01-15T01:10:29.950129+00:00 [queued]>
[2023-01-15T03:10:42.217+0200] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: railway_v01.create_spark_engine manual__2023-01-15T01:10:29.950129+00:00 [queued]>
[2023-01-15T03:10:42.219+0200] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-15T03:10:42.220+0200] {taskinstance.py:1284} INFO - Starting attempt 1 of 6
[2023-01-15T03:10:42.221+0200] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-15T03:10:42.568+0200] {taskinstance.py:1304} INFO - Executing <Task(_PythonDecoratedOperator): create_spark_engine> on 2023-01-15 01:10:29.950129+00:00
[2023-01-15T03:10:42.602+0200] {standard_task_runner.py:55} INFO - Started process 58323 to run task
[2023-01-15T03:10:42.652+0200] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'railway_v01', 'create_spark_engine', 'manual__2023-01-15T01:10:29.950129+00:00', '--job-id', '120', '--raw', '--subdir', 'DAGS_FOLDER/fetching_api.py', '--cfg-path', '/var/folders/qr/k2__dmk5697g2jyd7w2tpyt80000gn/T/tmpn0iw_ghr']
[2023-01-15T03:10:42.667+0200] {standard_task_runner.py:83} INFO - Job 120: Subtask create_spark_engine
[2023-01-15T03:10:43.523+0200] {task_command.py:389} INFO - Running <TaskInstance: railway_v01.create_spark_engine manual__2023-01-15T01:10:29.950129+00:00 [running]> on host nhanchaus-macbook-air.local
[2023-01-15T03:10:44.017+0200] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Nhan_Chau
AIRFLOW_CTX_DAG_ID=railway_v01
AIRFLOW_CTX_TASK_ID=create_spark_engine
AIRFLOW_CTX_EXECUTION_DATE=2023-01-15T01:10:29.950129+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-01-15T01:10:29.950129+00:00
[2023-01-15T03:11:45.716+0200] {python.py:177} INFO - Done. Returned value was: <pyspark.sql.session.SparkSession object at 0x1137954b0>
[2023-01-15T03:11:46.167+0200] {xcom.py:629} ERROR - Object of type SparkSession is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2023-01-15T03:11:46.203+0200] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2294, in xcom_push
    XCom.set(
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/models/xcom.py", line 234, in set
    value = cls.serialize_value(
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/models/xcom.py", line 627, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/utils/json.py", line 176, in encode
    return super().encode(o)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/airflow/utils/json.py", line 170, in default
    return super().default(o)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type SparkSession is not JSON serializable
[2023-01-15T03:11:46.342+0200] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=railway_v01, task_id=create_spark_engine, execution_date=20230115T011029, start_date=20230115T011042, end_date=20230115T011146
[2023-01-15T03:11:46.699+0200] {standard_task_runner.py:100} ERROR - Failed to execute job 120 for task create_spark_engine (Object of type SparkSession is not JSON serializable; 58323)
[2023-01-15T03:11:46.879+0200] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-15T03:11:48.132+0200] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
